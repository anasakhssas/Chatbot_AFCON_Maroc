"""
Script pour enrichir la base de donn√©es ChromaDB avec de nouvelles informations
Fusionne les fichiers JSON d'enrichissement avec les donn√©es existantes
"""

import json
import logging
from pathlib import Path
from datetime import datetime

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class DatabaseEnricher:
    """Classe pour enrichir la base de donn√©es avec de nouveaux documents"""
    
    def __init__(self):
        self.project_root = Path(__file__).parent.parent.parent
        self.enrichment_dir = self.project_root / "data" / "enrichment"
        self.transformed_dir = self.project_root / "data" / "transformed"
        self.combined_file = self.transformed_dir / "combined_dataset.json"
        
    def load_existing_data(self):
        """Charger les donn√©es existantes du fichier combin√©"""
        logger.info(f"üìÇ Chargement des donn√©es existantes : {self.combined_file}")
        
        if not self.combined_file.exists():
            logger.warning("‚ö†Ô∏è  Aucun fichier combin√© existant, cr√©ation d'un nouveau")
            return {
                "metadata": {
                    "source": "combined",
                    "date": datetime.now().strftime("%Y-%m-%d"),
                    "version": "1.0"
                },
                "documents": []
            }
        
        with open(self.combined_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        logger.info(f"‚úÖ {len(data['documents'])} documents existants charg√©s")
        return data
    
    def load_enrichment_files(self):
        """Charger tous les fichiers JSON du dossier enrichment"""
        logger.info(f"üìÇ Recherche de fichiers d'enrichissement dans : {self.enrichment_dir}")
        
        if not self.enrichment_dir.exists():
            logger.error(f"‚ùå Dossier introuvable : {self.enrichment_dir}")
            return []
        
        enrichment_files = list(self.enrichment_dir.glob("*.json"))
        logger.info(f"üìÑ {len(enrichment_files)} fichiers trouv√©s")
        
        all_documents = []
        for file_path in enrichment_files:
            logger.info(f"   üì• Chargement : {file_path.name}")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    documents = data.get('documents', [])
                    all_documents.extend(documents)
                    logger.info(f"      ‚úÖ {len(documents)} documents ajout√©s")
            except Exception as e:
                logger.error(f"      ‚ùå Erreur : {e}")
        
        logger.info(f"‚úÖ Total : {len(all_documents)} nouveaux documents charg√©s")
        return all_documents
    
    def remove_duplicates(self, existing_docs, new_docs):
        """Supprimer les doublons bas√©s sur l'ID"""
        logger.info("üîÑ Suppression des doublons...")
        
        # Construire le set d'IDs existants
        existing_ids = set()
        for doc in existing_docs:
            # L'ID peut √™tre dans metadata.id ou directement dans le doc
            if 'metadata' in doc and 'id' in doc['metadata']:
                existing_ids.add(doc['metadata']['id'])
            elif 'id' in doc:
                existing_ids.add(doc['id'])
        
        logger.info(f"   üìä {len(existing_ids)} IDs existants")
        
        unique_new_docs = []
        duplicates = 0
        
        for doc in new_docs:
            # R√©cup√©rer l'ID du nouveau document
            doc_id = doc.get('id') or doc.get('metadata', {}).get('id')
            
            if doc_id and doc_id not in existing_ids:
                unique_new_docs.append(doc)
                existing_ids.add(doc_id)
            else:
                duplicates += 1
        
        logger.info(f"   ‚úÖ {len(unique_new_docs)} documents uniques")
        if duplicates > 0:
            logger.info(f"   ‚ö†Ô∏è  {duplicates} doublons ignor√©s")
        
        return unique_new_docs
    
    def merge_and_save(self):
        """Fusionner les donn√©es existantes avec les nouvelles et sauvegarder"""
        logger.info("üöÄ D√©marrage de l'enrichissement de la base de donn√©es")
        logger.info("=" * 60)
        
        # Charger les donn√©es
        existing_data = self.load_existing_data()
        new_documents = self.load_enrichment_files()
        
        if not new_documents:
            logger.warning("‚ö†Ô∏è  Aucun nouveau document √† ajouter")
            return
        
        # Supprimer les doublons
        unique_new_docs = self.remove_duplicates(
            existing_data['documents'], 
            new_documents
        )
        
        if not unique_new_docs:
            logger.warning("‚ö†Ô∏è  Tous les documents sont d√©j√† pr√©sents")
            return
        
        # Fusionner
        logger.info("üîÑ Fusion des donn√©es...")
        existing_data['documents'].extend(unique_new_docs)
        
        # Mettre √† jour les m√©tadonn√©es
        existing_data['metadata']['date'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        existing_data['metadata']['last_enrichment'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        existing_data['metadata']['total_documents'] = len(existing_data['documents'])
        
        # Sauvegarder
        logger.info(f"üíæ Sauvegarde dans : {self.combined_file}")
        self.transformed_dir.mkdir(parents=True, exist_ok=True)
        
        with open(self.combined_file, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        
        logger.info("‚úÖ Base de donn√©es enrichie avec succ√®s!")
        logger.info("=" * 60)
        logger.info(f"üìä STATISTIQUES FINALES :")
        logger.info(f"   ‚Ä¢ Documents existants : {len(existing_data['documents']) - len(unique_new_docs)}")
        logger.info(f"   ‚Ä¢ Nouveaux documents : {len(unique_new_docs)}")
        logger.info(f"   ‚Ä¢ TOTAL : {len(existing_data['documents'])}")
        logger.info("=" * 60)
        
        # Cr√©er un backup
        backup_file = self.transformed_dir / f"combined_dataset_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        logger.info(f"üíæ Backup cr√©√© : {backup_file.name}")
        
        return existing_data
    
    def get_statistics(self):
        """Afficher les statistiques par cat√©gorie"""
        logger.info("\nüìä STATISTIQUES PAR CAT√âGORIE :")
        logger.info("=" * 60)
        
        data = self.load_existing_data()
        categories = {}
        
        for doc in data['documents']:
            category = doc['metadata'].get('category', 'non_class√©')
            categories[category] = categories.get(category, 0) + 1
        
        for category, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            logger.info(f"   ‚Ä¢ {category:30s} : {count:3d} documents")
        
        logger.info("=" * 60)


def main():
    """Point d'entr√©e principal"""
    print("\n" + "=" * 60)
    print("üöÄ ENRICHISSEMENT DE LA BASE DE DONN√âES ChromaDB")
    print("=" * 60 + "\n")
    
    enricher = DatabaseEnricher()
    
    # Fusionner et sauvegarder
    enricher.merge_and_save()
    
    # Afficher les statistiques
    enricher.get_statistics()
    
    print("\n" + "=" * 60)
    print("‚úÖ √âTAPE SUIVANTE :")
    print("   Ex√©cutez le script de vectorisation pour mettre √† jour ChromaDB :")
    print("   python src/pipeline/update_vectorstore.py")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
