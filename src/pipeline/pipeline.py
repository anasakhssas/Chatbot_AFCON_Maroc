"""Complete ETL pipeline: Extract -> Transform -> Load"""
import logging
from pathlib import Path
from .demo_scraper import save_demo_data
from .transform import DataTransformer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_complete_pipeline():
    """
    Run the complete ETL pipeline:
    1. Extract: Generate/scrape news data
    2. Transform: Prepare data for RAG
    3. Ready for Load: Data is ready for vector database
    """
    print("=" * 80)
    print("ğŸš€ PIPELINE COMPLET CAN 2025 - Extract â†’ Transform â†’ Ready for RAG")
    print("=" * 80)
    
    # Step 1: Extract (Demo Data)
    print("\nğŸ“¥ Ã‰TAPE 1: EXTRACTION DES DONNÃ‰ES")
    print("-" * 80)
    try:
        raw_data_path = save_demo_data()
        print(f"âœ… Extraction rÃ©ussie: {raw_data_path}")
    except Exception as e:
        print(f"âŒ Erreur lors de l'extraction: {e}")
        return
    
    # Step 2: Transform
    print("\nğŸ”„ Ã‰TAPE 2: TRANSFORMATION POUR RAG")
    print("-" * 80)
    try:
        transformer = DataTransformer()
        transformed_files = transformer.transform_all_files()
        
        if transformed_files:
            print(f"âœ… {len(transformed_files)} fichier(s) transformÃ©(s)")
            
            # Create combined dataset
            combined_path = transformer.create_combined_dataset()
            
            if combined_path:
                # Show statistics
                stats = transformer.get_statistics()
                print("\nğŸ“Š STATISTIQUES FINALES:")
                print(f"  â€¢ Fichiers bruts: {stats['raw_files']}")
                print(f"  â€¢ Fichiers transformÃ©s: {stats['transformed_files']}")
                print(f"  â€¢ Total documents: {stats['total_documents']}")
                
                if stats['categories']:
                    print("\n  ğŸ“‘ RÃ©partition par catÃ©gorie:")
                    for cat, count in stats['categories'].items():
                        print(f"    - {cat}: {count} documents")
                
                if stats['sources']:
                    print("\n  ğŸ“° RÃ©partition par source:")
                    for src, count in stats['sources'].items():
                        print(f"    - {src}: {count} documents")
                
                print(f"\nâœ… Dataset combinÃ© crÃ©Ã©: {combined_path}")
        else:
            print("âš ï¸ Aucune transformation effectuÃ©e")
    except Exception as e:
        print(f"âŒ Erreur lors de la transformation: {e}")
        return
    
    # Step 3: Ready for RAG
    print("\nâœ… Ã‰TAPE 3: DONNÃ‰ES PRÃŠTES POUR LE RAG")
    print("-" * 80)
    print("Les donnÃ©es transformÃ©es sont maintenant prÃªtes pour:")
    print("  â€¢ Vectorisation (embeddings)")
    print("  â€¢ Stockage dans ChromaDB")
    print("  â€¢ Utilisation avec LangChain/LlamaIndex")
    print("  â€¢ RequÃªtes du chatbot RAG")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ PIPELINE TERMINÃ‰ AVEC SUCCÃˆS!")
    print("=" * 80)
    print(f"\nğŸ“‚ DonnÃ©es disponibles:")
    print(f"  â€¢ Brutes: data/daily_fetch/")
    print(f"  â€¢ TransformÃ©es: data/transformed/")
    print(f"  â€¢ Dataset combinÃ©: data/transformed/combined_dataset.json")
    print("\nğŸš€ Prochaine Ã©tape: ImplÃ©menter le systÃ¨me RAG avec ChromaDB")


if __name__ == "__main__":
    run_complete_pipeline()
