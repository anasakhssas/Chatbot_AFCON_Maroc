"""Complete ETL pipeline: Extract -> Transform -> Load"""
import logging
from pathlib import Path
# from .demo_scraper import save_demo_data  # DÃ‰SACTIVÃ‰ - Contient donnÃ©es fictives
from .transform import DataTransformer

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def run_complete_pipeline():
    """
    Run the complete ETL pipeline:
    1. Extract: Generate/scrape news data (DÃ‰SACTIVÃ‰ - utiliser real_scraper.py)
    2. Transform: Prepare data for RAG
    3. Ready for Load: Data is ready for vector database
    """
    print("=" * 80)
    print("ğŸš€ PIPELINE CAN 2025 - Transform â†’ Ready for RAG")
    print("=" * 80)
    
    # Step 1: Extract (DÃ‰SACTIVÃ‰ - utiliser real_scraper.py pour donnÃ©es rÃ©elles)
    print("\nğŸ“¥ Ã‰TAPE 1: EXTRACTION DES DONNÃ‰ES")
    print("-" * 80)
    print("âš ï¸  Extraction automatique dÃ©sactivÃ©e")
    print("ğŸ’¡ Utilisez: python -m src.pipeline.real_scraper")
    print("âœ… DonnÃ©es brutes dÃ©jÃ  disponibles dans data/daily_fetch/")
    
    # VÃ©rifier que des donnÃ©es existent
    data_dir = Path(__file__).parent.parent.parent / "data" / "daily_fetch"
    if not data_dir.exists() or not list(data_dir.glob("*.json")):
        print("\nâŒ ERREUR: Aucune donnÃ©e brute trouvÃ©e!")
        print("ğŸ’¡ ExÃ©cutez d'abord: python -m src.pipeline.real_scraper")
        return
    
    raw_data_path = str(data_dir)
    
    # Step 2: Transform
    print("\nğŸ”„ Ã‰TAPE 2: TRANSFORMATION POUR RAG")
    print("-" * 80)
    try:
        transformer = DataTransformer()
        transformed_files = transformer.transform_all_files()
        
        if transformed_files:
            print(f"âœ… {len(transformed_files)} fichier(s) transformÃ©(s)")
            
            # Create combined dataset
            combined_path = transformer.create_combined_dataset()
            
            if combined_path:
                # Show statistics
                stats = transformer.get_statistics()
                print("\nğŸ“Š STATISTIQUES FINALES:")
                print(f"  â€¢ Fichiers bruts: {stats['raw_files']}")
                print(f"  â€¢ Fichiers transformÃ©s: {stats['transformed_files']}")
                print(f"  â€¢ Total documents: {stats['total_documents']}")
                
                if stats['categories']:
                    print("\n  ğŸ“‘ RÃ©partition par catÃ©gorie:")
                    for cat, count in stats['categories'].items():
                        print(f"    - {cat}: {count} documents")
                
                if stats['sources']:
                    print("\n  ğŸ“° RÃ©partition par source:")
                    for src, count in stats['sources'].items():
                        print(f"    - {src}: {count} documents")
                
                print(f"\nâœ… Dataset combinÃ© crÃ©Ã©: {combined_path}")
        else:
            print("âš ï¸ Aucune transformation effectuÃ©e")
    except Exception as e:
        print(f"âŒ Erreur lors de la transformation: {e}")
        return
    
    # Step 3: Ready for RAG
    print("\nâœ… Ã‰TAPE 3: DONNÃ‰ES PRÃŠTES POUR LE RAG")
    print("-" * 80)
    print("Les donnÃ©es transformÃ©es sont maintenant prÃªtes pour:")
    print("  â€¢ Vectorisation (embeddings)")
    print("  â€¢ Stockage dans ChromaDB")
    print("  â€¢ Utilisation avec LangChain/LlamaIndex")
    print("  â€¢ RequÃªtes du chatbot RAG")
    
    print("\n" + "=" * 80)
    print("ğŸ‰ PIPELINE TERMINÃ‰ AVEC SUCCÃˆS!")
    print("=" * 80)
    print(f"\nğŸ“‚ DonnÃ©es disponibles:")
    print(f"  â€¢ Brutes: data/daily_fetch/")
    print(f"  â€¢ TransformÃ©es: data/transformed/")
    print(f"  â€¢ Dataset combinÃ©: data/transformed/combined_dataset.json")
    print("\nğŸš€ Prochaine Ã©tape: ImplÃ©menter le systÃ¨me RAG avec ChromaDB")


if __name__ == "__main__":
    run_complete_pipeline()
